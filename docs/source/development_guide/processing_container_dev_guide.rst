.. _processing_container_dev_guide:

==================================
Developing a processing-container
==================================

A :term:`processing-container` is a container image, that processes data.
Such containers are commonly executed in tasks of a Workflow, e.g. for pre-pocessing, training or post-processing data.
We formulated a standard way for building processing-containers such that they are executable in Kaapana in the Task API.
This standard expects that a processing-container has to contain a `processing-container.json` file, that satisfies a json schema.
So make sure, that the Dockerfile for building the container image contains the following line:

.. code-block:: bash

    COPY files/processing-container.json /processing-container.json




The processing-container.json file
###################################

This file serves two purposes:

1. It communicates in a standardized way how to use this processing-container for data processing.
2. It contains all information that Kaapana needs to to execute as processing-container inside a task.

The json-schema for this file can be found here: TODO
Example processing-containers can be found here: TODO

A processing-container usually ships a specific tool and such a tool might support several usecases.
Therefore, the `processing-container.json` file can contain multiple task templates, that describe different usecases for the same tooling.

Each task template must contain the following information

Identifier 
----------
Users of this processing-container can declare which task template to use by specifying the corresponding identifier.


Description
------------
This should describe how this task template utilizes the tools in the container image to process data.
This can also contain high-level information about how the process expects input data to be structured and how results will be structured.


Environment variables
---------------------
It is a common concept that the execution of a container image can be cofigured via environment variables.
A task template should contain a list of configurable environment variables with descriptions on how they influence the processing of data.
An environment variable object must have the following fields:

* name
* value

It is highly recommended to also add the fields:

* description
* type
* choices
* adjustable

This will strongly improve the usability of the processing-container, as it ads clarity 
Furthermore, the Kaapana Web Interface can utilize these fields to show users how they can configure the task execution to their needs.


Input channels
--------------
Input data corresponds to the data that is processed during container runtime.
Many usecases require different types of data as input, e.g. image registration expects one fixed image and multiple moving images.
We assume, that the command that runs in the processing-container expects different types of data at different locations.
We understand each of these locations per data type as an input channels.

A task template must specify for each channel, where the data should be mounted inside the container.
As channels are identified by there name and need a description any input channel object requires the following fields:

* name
* mounted_path
* description

An additional feature provided by the Task API are scale rules for input channels.
You can specify how memory resources of the processing-container should be scaled based on the file sizes in your input channels.
More details will come TODO.


Output channels
------------------
Output data corresponds to all results that are generated during data-processing in form of files.
In order to distinguish different data types generated by the process we expect that generated files are structured according to their type,
e.g. the results of a training process usually consists of the trained model as well as logs from the training.
Similar to input channels a task template must specify one output channel for each data type created during the process.
Output channels consist of the same fields as input channels:

* name
* mounted_path
* description


Command [Optional]
-------------------
A container image can be shipped with multiple tools.
The command field specifies which command is executed for the corresponding that is executed in the container runtime as a list of strings.
If this is not specified the 

Resources [Optional]
---------------------
You can specify requests and limits of memory, CPU cores and GPU that the processing-container should use.


The Task API Command Line Interface (CLI)
#########################################

We provide a python CLI that allows to run and test processing-container locally with docker.
Hence, no Kaapana platform is needed for testing your processing-container.

Installation
------------

The task api package that contains the CLI can be installed via :ref:`pip`.

.. code:: bash

    python3 -m pip install task-api@git+https://codebase.helmholtz.cloud/kaapana/kaapana.git@develop#subdirectory=lib/task_api

Validating a processing-container.json file
-------------------------------------------

You can easily verify, if your processing-container.json file is complient with the schema.

python3 -m task_api.cli validate processing-container.json --schema pc

Running a task locally with Docker
-----------------------------------

For running a task that executes a processing-container locally you need to provide all required information in a `task.json` file.
The most important information you have to provide are:

* **image**: The image of the processing-container you want to execute in the task.
* **taskTemplate**: The identifier of the taskTemplate inside the `processing-container.json` file inside the container image.
* **inputs**: For each input channel in the task template you must provide a local paths to the directory with the input data.
* **outputs:** For each output channel in the task template you must provide a local path, where the output data should be persisted.

You can verify if your `task.json` file is valid via

.. code:: bash

    python3 -m task_api.cli validate task.json --schema task

If the task.json file is valid you can execute the task with

.. code:: bash

    python3 -m task_api.cli run task.json --mode docker

This will also create a `task_run-<id>.pkl` file relative to the current working directory.
This file can be used for subsequent commands like :code:`python3 -m task_api.cli logs task-run-<id>.pkl` to stream container logs to the current console.


To see more commands and functionalities of the CLI check :code:`python3 -m task_api.cli --help`.


Using a processing-container in an Airflow DAG
###############################################

To use a processing-container in an Airflow DAG it is necessary, that you have build the container image and pushed it to the default registry of your Kaapana platform.

For using a processing-container in an Airflow DAG we provide a dedicated :code:`KaapanaTaskOperator`.
So you don't have to write a dedicated Airflow operator anymore.

Currently, you have to install the `task-api-workflow` extension in your Kaapana platform to make this operator available.

The following is a minimal example for a DAG that consists of a single operator

.. code:: python

    from airflow.models import DAG
    from task_api_operator.KaapanaTaskOperator import KaapanaTaskOperator
    from kaapana.blueprints.kaapana_global_variables import (
        DEFAULT_REGISTRY,
        KAAPANA_BUILD_VERSION,
    )

    args = {
        "ui_visible": True,
        "owner": "kaapana",
    }

    with DAG("my-dag", default_args=args) as dag:
        my_task = KaapanaTaskOperator(
            task_id="my-task",
            image=f"{DEFAULT_REGISTRY}/<container-image>:{KAAPANA_BUILD_VERSION}",
            taskTemplate="my-tasktemplate-identifier",
        )

The :code:`KaapanaTaskOperator` requires at least three parameters to be set:

* :code:`task_id`: A unique name for the task in your DAG.
* :code:`image`: The container image of your processing-container which you pushed to the default registry of your Kaapana platform.
* :code:`taskTemplate`: The identifier of the task template that is specified in the :code:`processing-container.json` file in the container image.

Optional parameters allow you to overwrite environment variables and the command that is executed in the processing-container.


Passing data between operators
------------------------------

For passing data between two operators you have to set the parameter :code:`iochannel_maps` for the :code:`KaapanaTaskOperator:`.

Assume, you want to create a relatively simple workflow that consists of three tasks:

* **Task 1**: Based on the container image :code:`my-download` and the task template :code:`download-from-url`. It downloads data from an url into the output channel :code:`downloads`.
* **Task 2**: Based on the container image :code:`my-processing` and task template :code:`my-agorithm`. It processes all files in its input channel :code:`inputs` creates results and stores them in the output channel :code:`outputs`.
* **Task 3**: Based on the container image :code:`my-upload` and task template :code:`send-to-minio`. It sends all files in its input channel `inputs` to a Minio bucket.

In the parameter :code:`iochannel_maps` we can specify, which output channel should be mapped to which input channel.


.. code:: python

    from airflow.models import DAG
    from task_api_operator.KaapanaTaskOperator import KaapanaTaskOperator, IOMapping
    from kaapana.blueprints.kaapana_global_variables import (
        DEFAULT_REGISTRY,
        KAAPANA_BUILD_VERSION,
    )

    args = {
        "ui_visible": True,
        "owner": "kaapana",
    }

    with DAG("test-task-operator", default_args=args) as dag:
        download = KaapanaTaskOperator(
            task_id="get-data",
            image=f"{DEFAULT_REGISTRY}/my-get-data:{KAAPANA_BUILD_VERSION}",
            taskTemplate="download-from-url",
        )

        processing = KaapanaTaskOperator(
            task_id="process-data",
            image=f"{DEFAULT_REGISTRY}/my-processing:{KAAPANA_BUILD_VERSION}",
            taskTemplate="my-algorithm",
            iochannel_maps=[
                IOMapping(
                    upstream_operator=task1,
                    upstream_output_channel="downloads",
                    input_channel="inputs",
                )
            ],
        )

        upload = KaapanaTaskOperator(
            task_id="upload-data",
            image=f"{DEFAULT_REGISTRY}/my-upload:{KAAPANA_BUILD_VERSION}",
            taskTemplate="send-to-minio",
            iochannel_maps=[
                IOMapping(
                    upstream_operator=task2,
                    upstream_output_channel="results",
                    input_channel="inputs",
                )
            ],
        )

    download >> processing >> upload


You can find a hello-world example DAG that consists of two tasks here: TODO


Passing user configuration to a task-run
-----------------------------------------

A common requirement for workflows is, that users are able to make configurations to the processing of the data.
This configuration has to be passed to the process that is running inside the processing-containers.

Workflows are triggered via requests to the Airflow Rest API.
The payload of this request contains a :code:`conf` object, which is available to the :code:`KaapanaTaskOperator`.
You can configure environment variables in :code:`conf` at :code:`conf.task_form.{TASK_ID}.{VAR_NAME}.{VAR_VALUE}`
An example request to the Airflow Rest API to trigger a Workflow with custom configuration can look like this:

.. code:: bash

    curl -X 'POST' \
    'https://{KAAPANA_DOMAIN}/flow/api/v1/dags/{dag_id}/dagRuns' \
    -H 'accept: application/json' \
    -H 'Content-Type: application/json' \
    -d '{
    "dag_run_id": "<unique_dag_run_id>",
    "conf": {
        "task_form": {
            "{TASK_ID}": {
                "{VAR_NAME}": "{VAR_VALUE}" 
                }
            }
        }
    }'

This will pass the environment variable :code:`MY_VAR=MY_VAR_VALUE` to the container for the task_id :code:`TASK_ID` in the dag with dag-id :code:`dag_id`.

.. note::

    The order of precedence for environment variables is as follows:
    conf.task_form.env >> KaapanaTaskOperator.env >> processing-container.task-template.env 


Container images from another registry
----------------------------------------
In case you want to use a container image from another registry than the default registry, you can set the parameters :code:`registryUrl`, :code:`registryUsername`, :code:`registryPassword`.
This will create a dedicated registry secret for this task.



.. _data_structure_convention:

Input and output channel data structure convention
----------------------------------------------------

When data is passed from one task-run to another task-run,
the data structure of the output channel has to match the expectations of the respective input channels.
Therefore, we propose a conventional data structure for output channels.
We assume, that any channel contains results for 1 to N items.
Then we expect the output channel to have the following structure

.. code:: bash
    
    └── output-mount-path
        ├── item-1-identifier
        │   └── result
        ├── item-2-identifier
        │   └── result
        ├── ...
        └── item-N-identifier
            └── result

For processes that create results for each input item we expect, that item-identifiers of the output channel match the corresponding identifier from the input channel.

.. code:: bash
    
    └── input-mount-path
    │   ├── item-1-identifier
    │   │   └── input
    │   └── item-2-identifier
    │       └── input
    └── output-mount-path
        ├── item-1-identifier
        │   └── result
        └── item-2-identifier
            └── result

For processes that create a results on multiple items of an input-channel we expect an output channel with a single item.
The result-identifier should be different from input item identifiers.


.. code:: bash
    
    └── input-mount-path
    │   ├── item-1-identifier
    │   │   └── input
    │   └── item-2-identifier
    │       └── input
    └── output-mount-path
        └── result-identifier
            └── result


.. note::

    We strongly advise to use the description to specify which data structure is expected and can be exptected per input and output channel.


Migrating from KaapanaBaseOperator
####################################


This section will explain how to migrate the processing-container that was used in combination with an Airflow operator
which inerhited from the :code:`KaapanaBaseOperator` to a processing-container that can be used with the KaapanaTaskOperator.


Implicit conventions from the KaapanaBaseOperator
---------------------------------------------------

The :code:`KaapanaBaseOperator` has several implicit conventions that had to be considered, when implementing the processing-container, i.e. 

* Files are mounted into the same location for each processing-container.
* File locations have to be determined from generic environment variables.
* The generic environment variables are set automatically by the :code:`KaapanaBaseOperator`.
* Environment variables are globally set for all tasks in a workflow.

The :code:`KaapanaTaskOperator` will not automatically set the necessary environment variables to generate the same file paths as before.


.. list-table::
    :header-rows: 1

    *   - :code:`KaapanaBaseOperator`
        - :code:`KaapanaTaskOperator`
    *   - All workflow directories are mounted into the container.
        - Only input and output channels are mounted into the container.
    *   - File paths have to be constructed from environment variables.
        - File paths are always relative to :code:`mount_path` of the corresponding channel.
    *   - Environment variables for file path construction are set automatically.
        - No additional environment variables are set.
    *   - Variables in the :code:`conf` are shared between all containers in a workflow-run.
        - Variables in the :code:`conf` are task-run specific.

The biggest change is how data is mounted into the container.
If you use the KaapanaBaseOperator, every container will see a directory structure similar to

Assume you have the following DAG file

.. code:: python

    dag = DAG(dag_id="my_dag")

    get_input = GetInputOperator(dag=dag)
    my_algorithm = MyAlgorithmOperator(dag=dag, input_operator=get_input)

    get_input >> my_algorithm


Then Kaapana would automatically set the environment variables :code:`WORKFLOW_DIR`, :code:`BATCH_NAME`, :code:`OPERATOR_IN_DIR` and :code:`${OPERATOR_OUT_DIR}`.
Additionally, Kaapana would automatically mount the following directory structure in the container of the task-run of :code:`my_algorithm`

.. code:: bash

    └── ${WORKFLOW_DIR}
        ├── ${BATCH_NAME}
        │   ├── item-1
        │   │   └── ${OPERATOR_IN_DIR}
        │   │       └── input
        │   └── item-2
        │       └── ${OPERATOR_IN_DIR}
        │           └── input
        └── conf
            └── conf.json

It is expected, that :code:`my_algorithm` follows this convention. Hence, the final directory strucutre would look like this

.. code:: bash

    └── ${WORKFLOW_DIR}
        ├── ${BATCH_NAME}
        │   ├── item-1
        │   │   ├── ${OPERATOR_IN_DIR}
        │   │   │   └── input
        │   │   └── ${OPERATOR_OUT_DIR}
        │   │       └── result
        │   └── item-2
        │       ├── ${OPERATOR_IN_DIR}
        │       │   └── input
        │       └── ${OPERATOR_OUT_DIR}
        │           └── result
        └── conf
            └── conf.json


Migrating to new :ref:`data structure <data_structure_convention>`
-------------------------------------------------------------------

The DAG from above migrated to using the KaapanaTaskOperator could look like this:

.. code:: python

    with DAG("my_dag", default_args=args) as dag:
        get_input = KaapanaTaskOperator(
            task_id="get_input",
            image=f"{DEFAULT_REGISTRY}/get-input:{KAAPANA_BUILD_VERSION}",
            taskTemplate="dicom",
        )

        my_algorithm = KaapanaTaskOperator(
            task_id="my_algorithm",
            image=f"{DEFAULT_REGISTRY}/my-algorithm:{KAAPANA_BUILD_VERSION}",
            taskTemplate="my-algorithm",
            iochannel_maps=[
                IOMapping(
                    upstream_operator=get_input,
                    upstream_output_channel="downloads",
                    input_channel="inputs",
                )
            ],
        )


    get_input >> my_algorithm

Following the :ref:`data structure convetion <data_structure_convention> `, the directory structure in the processing-container of :code:`my_algorithm` should look like this:

.. code:: bash
    
    └── input-mount-path
    │   ├── item-1
    │   │   └── input
    │   └── item-2
    │       └── input
    └── output-mount-path
        ├── item-1
        │   └── result
        └── item-2
            └── result




Features not supported by the KaapanaTaskOperator
--------------------------------------------------

* The `kaapanapy` package does not work out of the box, as expected environment variables are not set automatically.
* The :code:`conf` object is not mounted into the container
* ui_forms: data_form, workflow_form