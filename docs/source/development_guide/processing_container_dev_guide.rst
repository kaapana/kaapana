.. _processing_container_dev_guide:

==================================
Developing a processing-container
==================================

A :term:`processing-container` is a container image, that processes data.
Such containers are commonly executed in tasks of a Workflow, e.g. for pre-pocessing, training or post-processing data.
We formulated a standward way for building processing-containers such that they are executable in Kaapana in the Task API.

These requirements are expressed in the `processing-container.json` file, which has to be part of any processing-container.
So make sure, that the Dockerfile for building the container image contains the following line:

.. code-block:: bash

    COPY files/processing-container.json /processing-container.json




The processing-container.json file
###################################

This file serves two purposes:

1. It communicates in a standardized way how to use this processing-container for data processing.
2. It contains all information that Kaapana needs to to execute as processing-container inside a task.

The json-schema for this file can be found here: TODO
Example processing-containers can be found here: TODO

A processing-container usually ships a specific tool and such a tool might support several usecases.
Therefore, the `processing-container.json` file can contain multiple task templates, that describe different usecases for the same tooling.

Each task template must contain the following information

Identifier 
----------
Users of this processing-container can declare which task template to use by specifying the corresponding identifier.


Description
------------
This should describe how this task template utilizes the tools in the container image to process data.
This can also contain high-level information about how the process expects input data to be structured and how results will be structured.


Environment variables
---------------------
It is a common concept that the execution of a container image can be cofigured via environment variables.
A task template should contain a list of configurable environment variables with descriptions on how they influence the processing of data.
An environment variable object must have the following fields:

* name
* value

It is highly recommended to also add the fields:

* description
* type
* choices
* adjustable

This will strongly improve the usability of the processing-container, as it ads clarity 
Furthermore, the Kaapana Web Interface can utilize these fields to show users how they can configure the task execution to their needs.


Input channels
--------------
Input data corresponds to the data that is processed during container runtime.
Many usecases require different types of data as input, e.g. image registration expects one fixed image and multiple moving images.
We assume, that the command that runs in the processing-container expects different types of data at different locations.
We understand each of these locations per data type as an input channels.

A task template must specify for each channel, where the data should be mounted inside the container.
As channels are identified by there name and need a description any input channel object requires the following fields:

* name
* mounted_path
* description

An additional feature provided by the Task API are scale rules for input channels.
You can specify how memory resources of the processing-container should be scaled based on the file sizes in your input channels.
More details will come TODO.


Output channels
------------------
Output data corresponds to all results that are generated during data-processing in form of files.
In order to distinguish different data types generated by the process we expect that generated files are structured according to their type,
e.g. the results of a training process usually consists of the trained model as well as logs from the training.
Similar to input channels a task template must specify one output channel for each data type created during the process.
Output channels consist of the same fields as input channels:

* name
* mounted_path
* description


Command [Optional]
-------------------
A container image can be shipped with multiple tools.
The command field specifies which command is executed for the corresponding that is executed in the container runtime as a list of strings.
If this is not specified the 

Resources [Optional]
---------------------
You can specify requests and limits of memory, CPU cores and GPU that the processing-container should use.


The Task API Command Line Interface (CLI)
#########################################

We provide a python CLI that allows to run and test processing-container locally with docker.
Hence, no Kaapana platform is needed for testing your processing-container.

Installation
------------

The task api package that contains the CLI can be installed via :ref:`pip`.

.. code:: bash

    python3 -m pip install task-api@git+https://codebase.helmholtz.cloud/kaapana/kaapana.git@develop#subdirectory=lib/task_api

Validating a processing-container.json file
-------------------------------------------

You can easily verify, if your processing-container.json file is complient with the schema.

python3 -m task_api.cli validate processing-container.json --schema pc

Running a task locally with Docker
-----------------------------------

For running a task that executes a processing-container locally you need to provide all required information in a `task.json` file.
The most important information you have to provide are:

* **image**: The image of the processing-container you want to execute in the task.
* **taskTemplate**: The identifier of the taskTemplate inside the `processing-container.json` file inside the container image.
* **inputs**: For each input channel in the task template you must provide a local paths to the directory with the input data.
* **outputs:** For each output channel in the task template you must provide a local path, where the output data should be persisted.

You can verify if your `task.json` file is valid via

.. code:: bash

    python3 -m task_api.cli validate task.json --schema task

If the task.json file is valid you can execute the task with

.. code:: bash

    python3 -m task_api.cli run task.json --mode docker

This will also create a `task_run-<id>.pkl` file relative to the current working directory.
This file can be used for subsequent commands like :code:`python3 -m task_api.cli logs task-run-<id>.pkl` to stream container logs to the current console.


To see more commands and functionalities of the CLI check :code:`python3 -m task_api.cli --help`.



Using a processing-container in an Airflow DAG
###############################################


* Install task-api-extension
* No need to define a custom operator
* Just use the KaapanaTaskOperator within the DAG
* How to specify IOChannelMap
* Link to example DAG

=======================================
Migrating a legacy processing-container
=======================================

* State of legacy processing-containers
    * Required implicit directory structure
    * Paths depend on environment variables
    * Configuration either via environment variables or workflow_config
    * Requires dedicated Airflow operator per processing-container