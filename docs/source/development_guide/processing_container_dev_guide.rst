.. _processing_container_dev_guide:

==================================
Developing a processing-container
==================================

A :term:`processing-container` is a container image, that processes data.
Such containers are commonly executed in tasks of a Workflow, e.g. for pre-pocessing, training or post-processing data.
We formulated a standward way for building processing-containers such that they are executable in Kaapana in the Task API.

These requirements are expressed in the `processing-container.json` file, which has to be part of any processing-container.
So make sure, that the Dockerfile for building the container image contains the following line:

.. code-block:: bash

    COPY files/processing-container.json /processing-container.json




The processing-container.json file
###################################

This file serves two purposes:

1. It communicates in a standardized way how to use this processing-container for data processing.
2. It contains all information that Kaapana needs to to execute as processing-container inside a task.

The json-schema for this file can be found here: TODO
Example processing-containers can be found here: TODO

A processing-container usually ships a specific tool and such a tool might support several usecases.
Therefore, the `processing-container.json` file can contain multiple task templates, that describe different usecases for the same tooling.

Each task template must contain the following information

Identifier 
----------
Users of this processing-container can declare which task template to use by specifying the corresponding identifier.


Description
------------
This should describe how this task template utilizes the tools in the container image to process data.
This can also contain high-level information about how the process expects input data to be structured and how results will be structured.


Environment variables
---------------------
It is a common concept that the execution of a container image can be cofigured via environment variables.
A task template should contain a list of configurable environment variables with descriptions on how they influence the processing of data.
An environment variable object must have the following fields:

* name
* value

It is highly recommended to also add the fields:

* description
* type
* choices
* adjustable

This will strongly improve the usability of the processing-container, as it ads clarity 
Furthermore, the Kaapana Web Interface can utilize these fields to show users how they can configure the task execution to their needs.


Input channels
--------------
Input data corresponds to the data that is processed during container runtime.
Many usecases require different types of data as input, e.g. image registration expects one fixed image and multiple moving images.
We assume, that the command that runs in the processing-container expects different types of data at different locations.
We understand each of these locations per data type as an input channels.

A task template must specify for each channel, where the data should be mounted inside the container.
As channels are identified by there name and need a description any input channel object requires the following fields:

* name
* mounted_path
* description

An additional feature provided by the Task API are scale rules for input channels.
You can specify how memory resources of the processing-container should be scaled based on the file sizes in your input channels.
More details will come TODO.


Output channels
------------------
Output data corresponds to all results that are generated during data-processing in form of files.
In order to distinguish different data types generated by the process we expect that generated files are structured according to their type,
e.g. the results of a training process usually consists of the trained model as well as logs from the training.
Similar to input channels a task template must specify one output channel for each data type created during the process.
Output channels consist of the same fields as input channels:

* name
* mounted_path
* description


Command [Optional]
-------------------
A container image can be shipped with multiple tools.
The command field specifies which command is executed for the corresponding that is executed in the container runtime as a list of strings.
If this is not specified the 

Resources [Optional]
---------------------
You can specify requests and limits of memory, CPU cores and GPU that the processing-container should use.


The Task API Command Line Interface (CLI)
#########################################

We provide a python CLI that allows to run and test processing-container locally with docker.
Hence, no Kaapana platform is needed for testing your processing-container.

Installation
------------

The task api package that contains the CLI can be installed via :ref:`pip`.

.. code:: bash

    python3 -m pip install task-api@git+https://codebase.helmholtz.cloud/kaapana/kaapana.git@develop#subdirectory=lib/task_api

Validating a processing-container.json file
-------------------------------------------

You can easily verify, if your processing-container.json file is complient with the schema.

python3 -m task_api.cli validate processing-container.json --schema pc

Running a task locally with Docker
-----------------------------------

For running a task that executes a processing-container locally you need to provide all required information in a `task.json` file.
The most important information you have to provide are:

* **image**: The image of the processing-container you want to execute in the task.
* **taskTemplate**: The identifier of the taskTemplate inside the `processing-container.json` file inside the container image.
* **inputs**: For each input channel in the task template you must provide a local paths to the directory with the input data.
* **outputs:** For each output channel in the task template you must provide a local path, where the output data should be persisted.

You can verify if your `task.json` file is valid via

.. code:: bash

    python3 -m task_api.cli validate task.json --schema task

If the task.json file is valid you can execute the task with

.. code:: bash

    python3 -m task_api.cli run task.json --mode docker

This will also create a `task_run-<id>.pkl` file relative to the current working directory.
This file can be used for subsequent commands like :code:`python3 -m task_api.cli logs task-run-<id>.pkl` to stream container logs to the current console.


To see more commands and functionalities of the CLI check :code:`python3 -m task_api.cli --help`.


Using a processing-container in an Airflow DAG
###############################################

To use a processing-container in an Airflow DAG it is necessary, that you have build the container image and pushed it to the default registry of your Kaapana platform.

For using a processing-container in an Airflow DAG we provide a dedicated :code:`KaapanaTaskOperator`.
So you don't have to write a dedicated Airflow operator anymore.

Currently, you have to install the `task-api-workflow` extension in your Kaapana platform to make this operator available.

The following is a minimal example for a DAG that consists of a single operator

.. code::python

    from airflow.models import DAG
    from task_api_operator.KaapanaTaskOperator import KaapanaTaskOperator
    from kaapana.blueprints.kaapana_global_variables import (
        DEFAULT_REGISTRY,
        KAAPANA_BUILD_VERSION,
    )

    args = {
        "ui_visible": True,
        "owner": "kaapana",
    }

    with DAG("my-dag", default_args=args) as dag:
        my_task = KaapanaTaskOperator(
            task_id="my-task",
            image=f"{DEFAULT_REGISTRY}/<container-image>:{KAAPANA_BUILD_VERSION}",
            taskTemplate="my-tasktemplate-identifier",
        )

The :code:`KaapanaTaskOperator` requires at least three parameters to be set:

* :code:`task_id`: A unique name for the task in your DAG.
* :code:`image`: The container image of your processing-container which you pushed to the default registry of your Kaapana platform.
* :code:`taskTemplate`: The identifier of the task template that is specified in the :code:`processing-container.json` file in the container image.

Optional parameters allow you to overwrite environment variables and the command that is executed in the processing-container.


Passing data between operators
------------------------------

For passing data between two operators you have to set the parameter :code:`iochannel_maps` for the :code:`KaapanaTaskOperator:`.

Assume, you want to create a relatively simple workflow that consists of three tasks:

* **Task 1**: Based on the container image :code:`my-download` and the task template :code:`download-from-url`. It downloads data from an url into the output channel :code:`downloads`.
* **Task 2**: Based on the container image :code:`my-processing` and task template :code:`my-agorithm`. It processes all files in its input channel :code:`inputs` creates results and stores them in the output channel :code:`outputs`.
* **Task 3**: Based on the container image :code:`my-upload` and task template :code:`send-to-minio`. It sends all files in its input channel `inputs` to a Minio bucket.

In the parameter :code:`iochannel_maps` we can specify, which output channel should be mapped to which input channel.


.. code::python
    from airflow.models import DAG
    from task_api_operator.KaapanaTaskOperator import KaapanaTaskOperator, IOMapping
    from kaapana.blueprints.kaapana_global_variables import (
        DEFAULT_REGISTRY,
        KAAPANA_BUILD_VERSION,
    )

    args = {
        "ui_visible": True,
        "owner": "kaapana",
    }

    with DAG("test-task-operator", default_args=args) as dag:
        download = KaapanaTaskOperator(
            task_id="get-data",
            image=f"{DEFAULT_REGISTRY}/my-get-data:{KAAPANA_BUILD_VERSION}",
            taskTemplate="download-from-url",
        )

        processing = KaapanaTaskOperator(
            task_id="process-data",
            image=f"{DEFAULT_REGISTRY}/my-processing:{KAAPANA_BUILD_VERSION}",
            taskTemplate="my-algorithm",
            iochannel_maps=[
                IOMapping(
                    upstream_operator=task1,
                    upstream_output_channel="downloads",
                    input_channel="inputs",
                )
            ],
        )

        upload = KaapanaTaskOperator(
            task_id="upload-data",
            image=f"{DEFAULT_REGISTRY}/my-upload:{KAAPANA_BUILD_VERSION}",
            taskTemplate="send-to-minio",
            iochannel_maps=[
                IOMapping(
                    upstream_operator=task2,
                    upstream_output_channel="results",
                    input_channel="inputs",
                )
            ],
        )

    download >> processing >> upload


You can find a hello-world example DAG that consists of two tasks here: TODO


Passing user configuration to a task
-------------------------------------

For many workflows you want to give the user the option to configure how the workflow is executed e.g. 

* Selecting a dataset that should be processed.
* 


Workflows are usually triggered via some API requests.



Using an image from another registry
------------------------------------
In case you want to use a container image from another registry than the default registry, you can set the parameters :code:`registryUrl`, :code:`registryUsername`, :code:`registryPassword`.
This will create a dedicated registry secret for this task.


Migrating a legacy processing-container
#########################################


* State of legacy processing-containers
    * Required implicit directory structure
    * Paths depend on environment variables
    * Configuration either via environment variables or workflow_config
    * Requires dedicated Airflow operator per processing-container