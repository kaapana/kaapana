---
apiVersion: v1
data:
  prometheus.yaml: |
    # my global config
    global:
      scrape_interval:     30s # Set the scrape interval to every 15 seconds. Default is every 1 minute.
      evaluation_interval: 30s # Evaluate rules every 15 seconds. The default is every 1 minute.
      # scrape_timeout is set to the global default (10s).


    # Load rules once and periodically evaluate them according to the global 'evaluation_interval'.
    rule_files:
      - "/etc/prometheus/alert.rules"
      # - "second_rules.yml"

    # A scrape configuration containing exactly one endpoint to scrape:
    # Here it's Prometheus itself.
    scrape_configs:
    {{- if .Values.global.gpu_support }}
      - job_name: 'GPUs'
        scheme: http
        metrics_path: '/metrics'
        static_configs:
        - targets: ['nvidia-dcgm-exporter.gpu-operator-resources.svc:9400']
    {{- end }}

      - job_name: 'Kaapana Node Info'
        scheme: http
        scrape_interval: 30s
        metrics_path: '/monitoring/metrics/scrape'
        static_configs:
        - targets: ['kaapana-backend-service.{{  .Values.global.services_namespace  }}.svc:5000']

      - job_name: 'prometheus'
        scheme: http
        metrics_path: '/prometheus/metrics'
        static_configs:
        - targets: ['prometheus-service.{{  .Values.global.services_namespace  }}.svc:9090']

      - job_name: 'oAuth2-proxy'
        scheme: https
        metrics_path: '/oauth2/metrics'
        static_configs:
        - targets: ['oauth2-proxy-service.{{  .Values.global.admin_namespace  }}.svc:8443']
        tls_config:
          insecure_skip_verify: true
      
      - job_name: 'kube-state-metrics'
        scheme: http
        metrics_path: '/metrics'
        static_configs:
        - targets: ['kube-state-metrics.{{  .Values.global.admin_namespace  }}.svc:8080']

      - job_name: 'Traefik'
        scheme: http
        scrape_interval: 30s
        metrics_path: '/metrics'
        static_configs:
      {{- if .Values.global.instance_uid }}
        - targets: ['traefik-{{  .Values.global.instance_uid | default 0 }}.{{  .Values.global.helm_namespace  }}.svc:8080']
      {{- else }}
        - targets: ['traefik-{{  .Values.global.instance_uid | default 0 }}.{{  .Values.global.admin_namespace  }}.svc:8080']
      {{- end }}

      - job_name: 'Granafa'
        scheme: http
        scrape_interval: 30s
        metrics_path: '/metrics'
        static_configs:
        - targets: ['grafana.{{  .Values.global.services_namespace  }}.svc:3000']

      - job_name: 'Airflow'
        scheme: http
        scrape_interval: 30s
        metrics_path: '/metrics'
        static_configs:
        - targets: ['airflow-statsd-service.{{  .Values.global.services_namespace  }}.svc:9102']

      - job_name: 'Node-Exporter'
        scheme: http
        scrape_interval: 15s
        metrics_path: '/metrics'
        static_configs:
        - targets: ['node-exporter.{{  .Values.global.services_namespace  }}.svc:9100']

      # Scrape config for nodes (kubelet).
      #
      # Rather than connecting directly to the node, the scrape is proxied though the
      # Kubernetes apiserver.  This means it will work if Prometheus is running out of
      # cluster, or can't connect to nodes for some other reason (e.g. because of
      # firewalling).
      - job_name: 'kubernetes-nodes'

        # Default to scraping over https. If required, just disable this or change to
        # `http`.
        scheme: https

        # This TLS & bearer token file config is used to connect to the actual scrape
        # endpoints for cluster components. This is separate to discovery auth
        # configuration because discovery & scraping are two separate concerns in
        # Prometheus. The discovery auth config is automatic if Prometheus runs inside
        # the cluster. Otherwise, more config options have to be provided within the
        # <kubernetes_sd_config>.
        tls_config:
          ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
        bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token

        kubernetes_sd_configs:
        - role: node

        relabel_configs:
        - action: labelmap
          regex: __meta_kubernetes_node_label_(.+)
        - target_label: __address__
          replacement: kubernetes.default.svc:443
        - source_labels: [__meta_kubernetes_node_name]
          regex: (.+)
          target_label: __metrics_path__
          replacement: /api/v1/nodes/${1}/proxy/metrics

      # Scrape config for Kubelet cAdvisor.
      #
      # This is required for Kubernetes 1.7.3 and later, where cAdvisor metrics
      # (those whose names begin with 'container_') have been removed from the
      # Kubelet metrics endpoint.  This job scrapes the cAdvisor endpoint to
      # retrieve those metrics.
      #
      # In Kubernetes 1.7.0-1.7.2, these metrics are only exposed on the cAdvisor
      # HTTP endpoint; use "replacement: /api/v1/nodes/${1}:4194/proxy/metrics"
      # in that case (and ensure cAdvisor's HTTP server hasn't been disabled with
      # the --cadvisor-port=0 Kubelet flag).
      #
      # This job is not necessary and should be removed in Kubernetes 1.6 and
      # earlier versions, or it will cause the metrics to be scraped twice.
      - job_name: 'kubernetes-cadvisor'

        # Default to scraping over https. If required, just disable this or change to
        # `http`.
        scheme: https

        # This TLS & bearer token file config is used to connect to the actual scrape
        # endpoints for cluster components. This is separate to discovery auth
        # configuration because discovery & scraping are two separate concerns in
        # Prometheus. The discovery auth config is automatic if Prometheus runs inside
        # the cluster. Otherwise, more config options have to be provided within the
        # <kubernetes_sd_config>.
        tls_config:
          ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
        bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token

        kubernetes_sd_configs:
        - role: node

        relabel_configs:
        - action: labelmap
          regex: __meta_kubernetes_node_label_(.+)
        - target_label: __address__
          replacement: kubernetes.default.svc:443
        - source_labels: [__meta_kubernetes_node_name]
          regex: (.+)
          target_label: __metrics_path__
          replacement: /api/v1/nodes/${1}/proxy/metrics/cadvisor

      - job_name: 'kubernetes-service-endpoints'

        kubernetes_sd_configs:
        - role: endpoints

        relabel_configs:
        - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scrape]
          action: keep
          regex: true
        - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scheme]
          action: replace
          target_label: __scheme__
          regex: (https?)
        - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_path]
          action: replace
          target_label: __metrics_path__
          regex: (.+)
        - source_labels: [__address__, __meta_kubernetes_service_annotation_prometheus_io_port]
          action: replace
          target_label: __address__
          regex: ([^:]+)(?::\d+)?;(\d+)
          replacement: $1:$2
        - action: labelmap
          regex: __meta_kubernetes_service_label_(.+)
        - source_labels: [__meta_kubernetes_namespace]
          action: replace
          target_label: kubernetes_namespace
        - source_labels: [__meta_kubernetes_service_name]
          action: replace
          target_label: kubernetes_name


    alerting:
      alertmanagers:
      - static_configs:
        - targets:
          - alertmanager-service.{{  .Values.global.services_namespace  }}.svc:9093
kind: ConfigMap
metadata:
  name: prometheus-config
  labels:
    name: prometheus-config
  namespace: "{{ .Values.global.services_namespace }}"

---